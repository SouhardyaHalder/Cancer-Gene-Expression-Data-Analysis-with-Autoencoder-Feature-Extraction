{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.10.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     88  15091  311   21  -13  215   797  14538  9738  8529  ...  384.4  1582  \\\n",
      "0   283  11038  134  -21 -219  116   433    615   115  1518  ...    231   624   \n",
      "1   309  16692  378   67  104  476  1474   5669  3272  3668  ...    720   753   \n",
      "2    12  15763  268   43 -148  155   415   4850  2293  2569  ...    307   743   \n",
      "3   168  18128  118   -8  -55  122   483   1284  2731   316  ...    178   626   \n",
      "4    71  34207  154  -24 -327  176   412   4148  2743  2404  ...    384  1157   \n",
      "..  ...    ...  ...  ...  ...  ...   ...    ...   ...   ...  ...    ...   ...   \n",
      "66  141  22818   46   26 -203   25   264    104  -159   120  ...    222   588   \n",
      "67   95  39323   73   39  -60   60   306    569   478   130  ...     60   669   \n",
      "68  146  15689  302   25 -209  183   657   3762  2164  1076  ...    288   745   \n",
      "69  431  41570  235   27 -626 -249   477   -159  -745  -834  ...    761   878   \n",
      "70    9  39538  101  106 -240  113  1313     34   -62  -308  ...    366   625   \n",
      "\n",
      "    185.4  511.4  389.6  793.2  329.8  36.6  191.4  -37.2  \n",
      "0     169    837    442    782    295    11     76    -14  \n",
      "1     315   1199    168   1138    777    41    228    -41  \n",
      "2     240    835    174    627    170   -50    126    -91  \n",
      "3     156    649    504    250    314    14     56    -25  \n",
      "4     115   1221    172    645    341    26    193    -53  \n",
      "..    ...    ...    ...    ...    ...   ...    ...    ...  \n",
      "66     92    532    239    707    354   -22    260      5  \n",
      "67     63    297    358    423     41     0   1777    -49  \n",
      "68    130    639    548    809    445    -2    210     16  \n",
      "69     84   1141    197    466    349     0    284    -73  \n",
      "70     81    574    618    551    194    20    379    -60  \n",
      "\n",
      "[71 rows x 5147 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset=pd.read_csv(\"F:\\AutoEncoder\\leukaemia_edited.csv\")\n",
    "print(dataset)\n",
    "X_train, X_test, = train_test_split(dataset,test_size=0.1)\n",
    "#12345678910"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          88     15091       311        21       -13       215       797  \\\n",
      "0   0.436831  0.225215  0.608752  0.344262  0.443332  0.186996  0.089684   \n",
      "1   0.738758  0.247216  0.676963  0.229508  0.484447  0.301411  0.168999   \n",
      "2   0.584582  0.350553  0.546976  0.513661  0.449554  0.241935  0.213582   \n",
      "3   0.293362  0.197916  0.678250  0.486339  0.440357  0.217238  0.062468   \n",
      "4   0.925054  0.588431  0.648649  0.295082  0.384366  0.162298  0.055988   \n",
      "..       ...       ...       ...       ...       ...       ...       ...   \n",
      "65  0.072805  0.360916  0.803089  0.639344  0.414120  0.255040  0.114826   \n",
      "66  0.000000  0.578803  0.751609  0.923497  0.441980  0.140625  0.118455   \n",
      "67  0.096360  0.639142  0.586873  0.967213  0.393292  0.182460  0.304821   \n",
      "68  0.773019  0.210347  0.824968  0.327869  0.806329  0.533770  0.321669   \n",
      "69  0.749465  0.234243  0.679537  0.218579  0.493373  0.209173  0.085796   \n",
      "\n",
      "       14538      9738      8529  ...     384.4      1582     185.4     511.4  \\\n",
      "0   0.069227  0.195765  0.042853  ...  0.155920  0.133577  0.454352  0.342932   \n",
      "1   0.074365  0.081831  0.072030  ...  0.330598  0.060742  0.558386  0.225785   \n",
      "2   0.015185  0.039818  0.020085  ...  0.098476  0.424634  0.503185  0.243455   \n",
      "3   0.033443  0.047139  0.035326  ...  0.528722  0.085208  0.675159  0.578534   \n",
      "4   0.007753  0.017065  0.021985  ...  0.304807  0.093926  0.849257  0.553010   \n",
      "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
      "65  0.190797  0.129139  0.079483  ...  0.468933  0.118391  0.736730  0.329843   \n",
      "66  0.820259  1.000000  0.250373  ...  0.218054  0.344769  0.397028  0.610602   \n",
      "67  0.011882  0.038466  0.019601  ...  0.376319  0.133296  0.295117  0.293848   \n",
      "68  0.110469  0.129984  0.063497  ...  0.239156  0.068054  0.433121  0.236257   \n",
      "69  0.000000  0.003830  0.024482  ...  0.109027  0.010967  0.407643  0.369764   \n",
      "\n",
      "       389.6     793.2     329.8      36.6     191.4     -37.2  \n",
      "0   0.206978  0.081406  0.309296  0.686335  0.038025  0.615672  \n",
      "1   0.163073  0.202847  0.537793  0.562112  0.084637  0.444030  \n",
      "2   0.103881  0.166370  0.122502  0.618012  0.045385  0.664179  \n",
      "3   0.063897  0.341192  0.520417  0.605590  0.123582  0.466418  \n",
      "4   0.103489  0.464413  0.435274  0.618012  0.043238  0.555970  \n",
      "..       ...       ...       ...       ...       ...       ...  \n",
      "65  0.127793  0.770463  0.423979  0.826087  0.260350  0.783582  \n",
      "66  0.078401  0.244662  0.328410  0.670807  0.095370  0.843284  \n",
      "67  0.251666  0.215302  0.205039  0.704969  0.137075  0.485075  \n",
      "68  0.140337  0.741103  0.300608  0.680124  1.000000  0.585821  \n",
      "69  0.139161  0.362544  0.265856  0.869565  0.117449  0.735075  \n",
      "\n",
      "[70 rows x 5147 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scale_datasets(X_train, X_test):\n",
    "  standard_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "  X_train_scaled = pd.DataFrame(\n",
    "      standard_scaler.fit_transform(X_train),\n",
    "      columns=X_train.columns\n",
    "  )\n",
    "  X_test_scaled = pd.DataFrame(\n",
    "      standard_scaler.transform(X_test),\n",
    "      columns = X_test.columns\n",
    "  )\n",
    "  return X_train_scaled, X_test_scaled\n",
    "X_train_scaled,X_test_scaled=scale_datasets(X_train,X_test) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          88     15091       311        21       -13       215       797  \\\n",
      "0   0.373626  0.266975  0.552124  0.071038  0.428996  0.194052  0.108605   \n",
      "1   0.386813  0.707699  0.536680  0.491803  0.377333  0.095766  0.000000   \n",
      "2   0.753846  0.475389  0.666667  0.770492  0.502299  0.231351  0.160705   \n",
      "3   0.773626  0.760498  1.000000  0.125683  0.405464  0.193548  0.033696   \n",
      "4   0.362637  0.315888  0.516088  0.530055  0.403300  0.138105  0.032919   \n",
      "..       ...       ...       ...       ...       ...       ...       ...   \n",
      "58  0.531868  0.000000  0.676963  0.224044  0.431972  0.351310  0.695956   \n",
      "59  0.714286  0.145986  0.583012  0.972678  0.397079  0.259073  0.174184   \n",
      "60  0.430769  0.823699  0.501931  0.901639  0.423857  0.168851  0.005962   \n",
      "61  0.767033  0.210347  0.824968  0.327869  0.806329  0.533770  0.321669   \n",
      "62  0.731868  0.247216  0.676963  0.229508  0.484447  0.301411  0.168999   \n",
      "\n",
      "       14538      9738      8529  ...     384.4      1582     185.4     511.4  \\\n",
      "0   0.009175  0.042619  0.025339  ...  0.167644  0.380484  0.577495  0.428297   \n",
      "1   0.001193  0.022230  0.015576  ...  0.262603  0.168729  0.307856  0.423816   \n",
      "2   0.005551  0.031788  0.019601  ...  0.089097  0.120079  0.373673  0.097951   \n",
      "3   0.012937  0.035894  0.018520  ...  0.572098  0.551462  0.711253  0.596671   \n",
      "4   0.015093  0.041487  0.035549  ...  0.207503  0.122891  0.318471  0.282330   \n",
      "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
      "58  0.013533  0.036248  0.017514  ...  0.432591  0.568898  0.600849  0.594750   \n",
      "59  0.005918  0.024637  0.016843  ...  1.000000  0.215411  0.634820  0.719590   \n",
      "60  0.008349  0.035894  0.014905  ...  0.200469  0.195444  0.537155  0.532010   \n",
      "61  0.110469  0.163398  0.063497  ...  0.239156  0.068054  0.433121  0.252881   \n",
      "62  0.074365  0.102867  0.072030  ...  0.330598  0.060742  0.558386  0.242638   \n",
      "\n",
      "       389.6     793.2     329.8      36.6     191.4     -37.2  \n",
      "0   0.074781  0.125000  0.217202  0.642857  0.043545  0.664179  \n",
      "1   0.097852  0.328737  0.333623  0.686335  0.094143  0.798507  \n",
      "2   0.178600  0.245107  0.304083  0.642857  0.080957  0.783582  \n",
      "3   0.079952  0.447954  0.449175  0.791925  0.072370  0.447761  \n",
      "4   0.089897  0.284698  0.344049  0.574534  0.100583  0.727612  \n",
      "..       ...       ...       ...       ...       ...       ...  \n",
      "58  0.077963  0.608541  0.258036  0.807453  0.187979  0.649254  \n",
      "59  0.096261  0.726868  1.000000  0.832298  0.182153  0.399254  \n",
      "60  0.114956  0.269573  0.217202  0.431677  0.090156  1.000000  \n",
      "61  0.127685  0.741103  0.300608  0.680124  1.000000  0.585821  \n",
      "62  0.150756  0.202847  0.537793  0.562112  0.084637  0.444030  \n",
      "\n",
      "[63 rows x 5147 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "63/63 [==============================] - 6s 81ms/step - loss: 0.0428 - mse: 0.0428 - val_loss: 0.0427 - val_mse: 0.0427\n",
      "Epoch 2/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0400 - mse: 0.0400 - val_loss: 0.0399 - val_mse: 0.0399\n",
      "Epoch 3/400\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.0401 - val_mse: 0.0401\n",
      "Epoch 4/400\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0394 - val_mse: 0.0394\n",
      "Epoch 5/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 6/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.0395 - val_mse: 0.0395\n",
      "Epoch 7/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 8/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.0394 - val_mse: 0.0394\n",
      "Epoch 9/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0394 - val_mse: 0.0394\n",
      "Epoch 10/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 11/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 12/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0397 - val_mse: 0.0397\n",
      "Epoch 13/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 14/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0388 - val_mse: 0.0388\n",
      "Epoch 15/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 16/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0389 - val_mse: 0.0389\n",
      "Epoch 17/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 18/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0388 - val_mse: 0.0388\n",
      "Epoch 19/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0388 - val_mse: 0.0388\n",
      "Epoch 20/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 21/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 22/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 23/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0388 - val_mse: 0.0388\n",
      "Epoch 24/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 25/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 26/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 27/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 28/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 29/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 30/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 31/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0394 - val_mse: 0.0394\n",
      "Epoch 32/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 33/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 34/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 35/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 36/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 37/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 38/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 39/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 40/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 41/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 42/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 43/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 44/400\n",
      "63/63 [==============================] - 4s 71ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 45/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0389 - val_mse: 0.0389\n",
      "Epoch 46/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 47/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 48/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0394 - val_mse: 0.0394\n",
      "Epoch 49/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 50/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 51/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 52/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 53/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 54/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 55/400\n",
      "63/63 [==============================] - 5s 76ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 56/400\n",
      "63/63 [==============================] - 4s 71ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0389 - val_mse: 0.0389\n",
      "Epoch 57/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 58/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 59/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 60/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 61/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 62/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 63/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 64/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 65/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 66/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 67/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 68/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 69/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 70/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 71/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 72/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 73/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 74/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 75/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 76/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 77/400\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 78/400\n",
      "63/63 [==============================] - 5s 80ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 79/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0389 - val_mse: 0.0389\n",
      "Epoch 80/400\n",
      "63/63 [==============================] - 5s 73ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 81/400\n",
      "63/63 [==============================] - 5s 73ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 82/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 83/400\n",
      "63/63 [==============================] - 5s 73ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 84/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 85/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 86/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 87/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 88/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 89/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 90/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 91/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 92/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 93/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 94/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 95/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 96/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 97/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 98/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 99/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 100/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 101/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 102/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 103/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 104/400\n",
      "63/63 [==============================] - 5s 76ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 105/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 106/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 107/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 108/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 109/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 110/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 111/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 112/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 113/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 114/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 115/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 116/400\n",
      "63/63 [==============================] - 5s 71ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 117/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 118/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 119/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 120/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 121/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 122/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 123/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 124/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 125/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 126/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 127/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 128/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0389 - val_mse: 0.0389\n",
      "Epoch 129/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 130/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 131/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0394 - val_mse: 0.0394\n",
      "Epoch 132/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 133/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 134/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 135/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 136/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 137/400\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 138/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 139/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 140/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 141/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0389 - val_mse: 0.0389\n",
      "Epoch 142/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 143/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 144/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 145/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 146/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 147/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 148/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 149/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 150/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 151/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0389 - val_mse: 0.0389\n",
      "Epoch 152/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 153/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 154/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 155/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0394 - val_mse: 0.0394\n",
      "Epoch 156/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 157/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 158/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 159/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 160/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 161/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0389 - val_mse: 0.0389\n",
      "Epoch 162/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 163/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 164/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 165/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 166/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 167/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 168/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 169/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 170/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 171/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 172/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 173/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 174/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 175/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 176/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 177/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 178/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 179/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 180/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 181/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 182/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 183/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 184/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 185/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0394 - val_mse: 0.0394\n",
      "Epoch 186/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 187/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 188/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 189/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 190/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 191/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 192/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 193/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 194/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 195/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 196/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 197/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 198/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 199/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 200/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 201/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 202/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 203/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 204/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 205/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 206/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 207/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 208/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 209/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 210/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 211/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 212/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 213/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 214/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 215/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 216/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 217/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 218/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 219/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 220/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 221/400\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 222/400\n",
      "63/63 [==============================] - 1479s 24s/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 223/400\n",
      "63/63 [==============================] - 4s 71ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 224/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 225/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 226/400\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 227/400\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 228/400\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 229/400\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 230/400\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 231/400\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 232/400\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 233/400\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 234/400\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 235/400\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 236/400\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 237/400\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 238/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 239/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 240/400\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 241/400\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 242/400\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 243/400\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 244/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 245/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 246/400\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 247/400\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 248/400\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 249/400\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 250/400\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0389 - val_mse: 0.0389\n",
      "Epoch 251/400\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 252/400\n",
      "63/63 [==============================] - 5s 73ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 253/400\n",
      "63/63 [==============================] - 5s 73ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 254/400\n",
      "63/63 [==============================] - 5s 73ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 255/400\n",
      "63/63 [==============================] - 5s 73ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 256/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 257/400\n",
      "63/63 [==============================] - 5s 73ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 258/400\n",
      "63/63 [==============================] - 5s 73ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 259/400\n",
      "63/63 [==============================] - 5s 73ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 260/400\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 261/400\n",
      "63/63 [==============================] - 5s 73ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 262/400\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 263/400\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 264/400\n",
      "63/63 [==============================] - 5s 73ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 265/400\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 266/400\n",
      "63/63 [==============================] - 5s 71ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 267/400\n",
      "63/63 [==============================] - 4s 71ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 268/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 269/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 270/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 271/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 272/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 273/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 274/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 275/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 276/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 277/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 278/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 279/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0389 - val_mse: 0.0389\n",
      "Epoch 280/400\n",
      "63/63 [==============================] - 5s 78ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 281/400\n",
      "63/63 [==============================] - 5s 73ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 282/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 283/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0389 - val_mse: 0.0389\n",
      "Epoch 284/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 285/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 286/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 287/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 288/400\n",
      "63/63 [==============================] - 5s 76ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 289/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0394 - val_mse: 0.0394\n",
      "Epoch 290/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 291/400\n",
      "63/63 [==============================] - 5s 76ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 292/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0394 - val_mse: 0.0394\n",
      "Epoch 293/400\n",
      "63/63 [==============================] - 5s 76ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 294/400\n",
      "63/63 [==============================] - 5s 76ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 295/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 296/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 297/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 298/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 299/400\n",
      "63/63 [==============================] - 5s 76ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 300/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0394 - val_mse: 0.0394\n",
      "Epoch 301/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 302/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 303/400\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 304/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 305/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 306/400\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 307/400\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 308/400\n",
      "63/63 [==============================] - 5s 73ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 309/400\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 310/400\n",
      "63/63 [==============================] - 5s 73ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 311/400\n",
      "63/63 [==============================] - 5s 73ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 312/400\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 313/400\n",
      "63/63 [==============================] - 5s 73ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0395 - val_mse: 0.0395\n",
      "Epoch 314/400\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 315/400\n",
      "63/63 [==============================] - 5s 71ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 316/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 317/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 318/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 319/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 320/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0389 - val_mse: 0.0389\n",
      "Epoch 321/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 322/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 323/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0394 - val_mse: 0.0394\n",
      "Epoch 324/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 325/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 326/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 327/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 328/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 329/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0394 - val_mse: 0.0394\n",
      "Epoch 330/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 331/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0394 - val_mse: 0.0394\n",
      "Epoch 332/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0389 - val_mse: 0.0389\n",
      "Epoch 333/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 334/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 335/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 336/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 337/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 338/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 339/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 340/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 341/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 342/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 343/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 344/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 345/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 346/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 347/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0389 - val_mse: 0.0389\n",
      "Epoch 348/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 349/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 350/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 351/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 352/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 353/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0388 - val_mse: 0.0388\n",
      "Epoch 354/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 355/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 356/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 357/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 358/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 359/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 360/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 361/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 362/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 363/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 364/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 365/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0394 - val_mse: 0.0394\n",
      "Epoch 366/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 367/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 368/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 369/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 370/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 371/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 372/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 373/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 374/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0394 - val_mse: 0.0394\n",
      "Epoch 375/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 376/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 377/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 378/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 379/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 380/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 381/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 382/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 383/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 384/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 385/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 386/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 387/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 388/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 389/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 390/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 391/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 392/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 393/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 394/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 395/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 396/400\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 397/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 398/400\n",
      "63/63 [==============================] - 4s 68ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0392 - val_mse: 0.0392\n",
      "Epoch 399/400\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 400/400\n",
      "63/63 [==============================] - 4s 70ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0390 - val_mse: 0.0390\n"
     ]
    }
   ],
   "source": [
    "class AutoEncoders(Model):\n",
    "\n",
    "  def __init__(self, output_units):\n",
    "\n",
    "    super().__init__()\n",
    "    \n",
    "    self.encoder = Sequential(\n",
    "        [\n",
    "          Dense(1000, activation=\"tanh\"),\n",
    "          Dense(500, activation=\"tanh\"),\n",
    "          Dense(100, activation=\"tanh\"),\n",
    "          Dense(50, activation=\"tanh\"),\n",
    "          Dense(15, activation=\"tanh\")\n",
    "        ]\n",
    "    )\n",
    "    self.decoder = Sequential(\n",
    "        [\n",
    "            Dense(50, activation=\"sigmoid\"),\n",
    "          Dense(100, activation=\"sigmoid\"),\n",
    "          Dense(500, activation=\"sigmoid\"),\n",
    "          Dense(1000, activation=\"sigmoid\"),\n",
    "          Dense(output_units, activation=\"sigmoid\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "  def call(self, inputs):\n",
    "\n",
    "    encoded = self.encoder(inputs)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "auto_encoder = AutoEncoders(len(X_train_scaled.columns))\n",
    "\n",
    "auto_encoder.compile(\n",
    "    loss='mse',\n",
    "    metrics=['mse'],\n",
    "    optimizer='adam'\n",
    "    \n",
    "  )\n",
    "history = auto_encoder.fit(\n",
    "      X_train_scaled, \n",
    "      X_train_scaled,\n",
    "      epochs=400, \n",
    "      batch_size=1, \n",
    "      validation_data=(X_test_scaled, X_test_scaled)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No such layer: sequential_2. Existing layers are: ['sequential_8', 'sequential_9'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m encoder_layer \u001b[38;5;241m=\u001b[39m \u001b[43mauto_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msequential_2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m reduced_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(encoder_layer\u001b[38;5;241m.\u001b[39mpredict(X_train_scaled))\n\u001b[0;32m      3\u001b[0m reduced_df \u001b[38;5;241m=\u001b[39m reduced_df\u001b[38;5;241m.\u001b[39madd_prefix(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:3275\u001b[0m, in \u001b[0;36mModel.get_layer\u001b[1;34m(self, name, index)\u001b[0m\n\u001b[0;32m   3273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m name:\n\u001b[0;32m   3274\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m layer\n\u001b[1;32m-> 3275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3276\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such layer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Existing layers are: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(layer\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3278\u001b[0m     )\n\u001b[0;32m   3279\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3280\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvide either a layer name or layer index at \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`get_layer`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3281\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: No such layer: sequential_2. Existing layers are: ['sequential_8', 'sequential_9']."
     ]
    }
   ],
   "source": [
    "encoder_layer = auto_encoder.get_layer('sequential_2')\n",
    "reduced_df = pd.DataFrame(encoder_layer.predict(X_train_scaled))\n",
    "reduced_df = reduced_df.add_prefix('feature_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0    0.987121   0.973766   0.985543   0.248708   0.981070   0.998433   \n",
      "1    0.986936   0.973373   0.986025   0.268485   0.981555   0.998396   \n",
      "2    0.987456   0.974787   0.983966   0.180210   0.979671   0.998525   \n",
      "3    0.987444   0.974722   0.984019   0.182173   0.979788   0.998520   \n",
      "4    0.986650   0.972503   0.986981   0.316015   0.982650   0.998310   \n",
      "..        ...        ...        ...        ...        ...        ...   \n",
      "65   0.987452   0.974768   0.983860   0.175700   0.979733   0.998524   \n",
      "66   0.987550   0.975067   0.983362   0.155535   0.979256   0.998549   \n",
      "67   0.987544   0.975040   0.983338   0.155442   0.979317   0.998547   \n",
      "68   0.987111   0.973772   0.985557   0.246074   0.980986   0.998436   \n",
      "69   0.987516   0.975014   0.983483   0.160736   0.979373   0.998542   \n",
      "\n",
      "    feature_6  feature_7  feature_8  feature_9  feature_10  feature_11  \\\n",
      "0   -0.968814   0.996773  -0.968948   0.991416   -0.996788    0.971795   \n",
      "1   -0.968808   0.996691  -0.969205   0.991229   -0.996805    0.971727   \n",
      "2   -0.968536   0.997004  -0.967821   0.991897   -0.996807    0.972096   \n",
      "3   -0.968527   0.996985  -0.967796   0.991877   -0.996801    0.971993   \n",
      "4   -0.968701   0.996506  -0.969861   0.990889   -0.996809    0.971427   \n",
      "..        ...        ...        ...        ...         ...         ...   \n",
      "65  -0.968377   0.997008  -0.967595   0.991929   -0.996818    0.972066   \n",
      "66  -0.968324   0.997073  -0.967329   0.992053   -0.996818    0.972146   \n",
      "67  -0.968267   0.997067  -0.967250   0.992064   -0.996820    0.972107   \n",
      "68  -0.968842   0.996778  -0.968941   0.991396   -0.996793    0.971839   \n",
      "69  -0.968358   0.997059  -0.967433   0.992021   -0.996820    0.972134   \n",
      "\n",
      "    feature_12  feature_13  feature_14  \n",
      "0    -0.964871    0.995032    0.994657  \n",
      "1    -0.964500    0.995088    0.994815  \n",
      "2    -0.965990    0.994893    0.994145  \n",
      "3    -0.965969    0.994882    0.994157  \n",
      "4    -0.963683    0.995187    0.995146  \n",
      "..         ...         ...         ...  \n",
      "65   -0.966084    0.994885    0.994120  \n",
      "66   -0.966418    0.994834    0.993961  \n",
      "67   -0.966423    0.994827    0.993955  \n",
      "68   -0.964858    0.995044    0.994649  \n",
      "69   -0.966339    0.994845    0.994001  \n",
      "\n",
      "[70 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "print(reduced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5addf786bcd861d1ce5006f23111f8cbb206731e5b61b0a5632ba9e0252558a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
