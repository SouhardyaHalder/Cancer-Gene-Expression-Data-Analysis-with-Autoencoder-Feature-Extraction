{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.10.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    -156  -115  -140  149  -111  -334   -47  -204  -148   377  ...  172.9  \\\n",
      "0     16   -63   -89   92  -394  -719   290  -132   238   141  ...     56   \n",
      "1     34   -81   163  110  -230  -353   158  -114   -11    66  ...    200   \n",
      "2    -72   -58    60  170  -117  -328   264   -71    -2    55  ...     72   \n",
      "3   -552  -212  -191  374   -50  -404  -399   -61  -219    -6  ...     61   \n",
      "4     33  -110   -31   12  -267  -663   239  -238     3   195  ...    202   \n",
      "5    -76   -54    97   -6  -100  -541   488   -97    76    -1  ...    200   \n",
      "6   -159  -148  -474  595  -708  -744   708  -304   131   379  ...   1088   \n",
      "7   -106   -14    87  177  -305  -300    21  -106   -17   -46  ...      1   \n",
      "8   -432  -173  -101   65  -286  -505   888  -137   -10   -42  ...    111   \n",
      "9   -133  -119  -162   42  -290 -1150  1199  -166   124   -42  ...    110   \n",
      "10   -76  -289   180  -43  -732 -1294  1180  -650   229 -1152  ...    742   \n",
      "11  -433  -240   534  240  -539 -1010  -279  -346   -86 -1063  ...    284   \n",
      "12   -82  -141   -82  377  -354  -991 -1234  -797   -23  -183  ...    436   \n",
      "13  -790  -934  -190  451  -498 -1139  1090  -671    79   203  ...    247   \n",
      "14  -367  -291   -83  109  -702  -622  -924  -112  -112    83  ...    437   \n",
      "15   -36  -213  -218  544  -472  -853   925  -465    85   -51  ...    247   \n",
      "16  -184  -137  -334   52   -52  -406   912  -172   -56  -151  ...     82   \n",
      "17  -200  -352   341  368  -661 -1432  1085  -553  -141   884  ...    124   \n",
      "18    -4  -272   714  408  -653 -1226   930  -404  -225   -98  ...     32   \n",
      "19  -132  -211  -163  143  -250  -415  -144  -201  -123    40  ...    225   \n",
      "20   -82  -136  -148  121  -189  -327  -113   -61   -55   161  ...    390   \n",
      "21   -82  -181   -51   -8  -240  -206   119  -221   -48    67  ...    176   \n",
      "22  -208  -115  -223   81  -190  -587  -970  -195    55  1206  ...    286   \n",
      "23  -557  -326  -184  107  -242  -483  1318  -136   -73   339  ...    359   \n",
      "24  -114  -131  -251   67  -136  -433  -265  -124   119    16  ...     -7   \n",
      "25  -124  -131  -134  192   -96  -396  -377  -200    17   -43  ...    214   \n",
      "26  -117   -76  -126  163  -247  -303  -242  -159   -90   -48  ...    151   \n",
      "27   -25  -139  -135   -8  -128  -147   310  -105    26   -20  ...    139   \n",
      "28  -466  -441  -340  215  -551  -466   436  -361  -235   205  ...    270   \n",
      "29  -212  -200  -279  807  -910 -1208   868  -169   327   121  ...   -279   \n",
      "30 -1969 -1688   490  627  -972 -2218 -2475  -747   -64  -570  ...    610   \n",
      "31  -198  -314  -105  262  -695 -1073 -1346  -442  -112   288  ...    413   \n",
      "32 -1425 -1074   226  867  -426 -1343   591  -108   105   121  ...    259   \n",
      "33   -80    -4   291  318  -300  -568   -22  -264   300     0  ...     67   \n",
      "34   -72   -24  -268  122  -256  -428   345  -264   349   128  ...    -39   \n",
      "35  -394  -605   827  471  -673  -875  1654  -798   471  -923  ...   -413   \n",
      "36     8  -408    51  198  -408 -1046 -1016  -681  -212    35  ...    683   \n",
      "37   -89   -64    14  163  -143  -391  1010  -326  -163   336  ...    262   \n",
      "38  -106  -359    14  424  -548  -937  -932  -334  -151  -263  ...    116   \n",
      "\n",
      "    1483.1  -10.9  182.9  -16.3  472.1   1.7  27.16  51.6  -27.10  \n",
      "0     2490    -66   -151   -280   1484   165     56   328     -61  \n",
      "1     1742    154    445    -57    784   379    -58    36     -77  \n",
      "2     1042    -23    130    -88    605   319     28   -51     -80  \n",
      "3       84   -961   -712   -741    842    91   -627  -563    -751  \n",
      "4     1922    245     76    -61   1327   687    -18   193     -55  \n",
      "5     1264    -17    312    -24    717   418    -54    57     -67  \n",
      "6     2970    212    453   -265   1673   719    216   336     -31  \n",
      "7      693    -19    124     -8    135   174     -5   183     -53  \n",
      "8     1396   -284    133   -149    995   252   -136    50    -280  \n",
      "9     4429   -281     83    -58   3227  1574    112   173     -36  \n",
      "10    3288   -513    737     27   6544  1835   -125   284    -355  \n",
      "11    3826   -645   1410   -539   6960  1564   -255   697    -375  \n",
      "12    4404   -135    147   -271   3306  1440    -41   212     -70  \n",
      "13    3531    -86    250   -156   4195   832   -123   468    -213  \n",
      "14    5127  -1493   -721  -1223   1391  -225  -1030  -713   -1343  \n",
      "15    4665     -9    668     63   2104   681    -45   130     -74  \n",
      "16    3117   -214    121   -431   1414    89   -307   554    -222  \n",
      "17    6261   -265    575   -135   6543  2474     27   385     -16  \n",
      "18    3929     61    220   -244   5522  1659    -75   314     -84  \n",
      "19    1879   -220    355   -186    361   290   -225   -50    -190  \n",
      "20    2904    -53    291    -16    472   404     55   113     -60  \n",
      "21     985   -464   -153   -343    111    86   -359   -24    -337  \n",
      "22    2769     -1    324    -18    391   701     47   516     -30  \n",
      "23    1370   -182    227   -180    801   138   -145   274    -106  \n",
      "24    2694    -14    371    -38    -59   340    -28    95     -50  \n",
      "25    4632    -48    273    -33    432   277      1   182     -70  \n",
      "26    2385     23    349    -19    474   384     11    82     -26  \n",
      "27    6171      5    216    -32    284   316      5    13     -36  \n",
      "28    2286  -2156  -2231  -1850   -782 -1494  -1383 -1604   -1649  \n",
      "29    2732    461   1481   -309   1809   382   -133   552     -42  \n",
      "30    2419   -844   2974   -699   3287  1583   -691  -152    -450  \n",
      "31    3321     76   2753   -432   3683  1724   -169   250     -83  \n",
      "32    5468  -1093   2588   -640   6276  2509   -574   426    -670  \n",
      "33    2275    -71    286   -120    430   313     71   282    -165  \n",
      "34    2084     24    355    -90    521   233    -31    41     -55  \n",
      "35    4058    211     28    134   4058   461     19   884    -375  \n",
      "36    6792    -81    890   -223   2141   929    136   569     -89  \n",
      "37    2571    -59   -128   -252    198    59   -123   -34    -203  \n",
      "38    1791     89    218     59   1076   727     37   -64    -104  \n",
      "\n",
      "[39 rows x 7129 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset=pd.read_csv(\"F:/AutoEncoder/rain_tumour_edited.csv\")\n",
    "print(dataset)\n",
    "X_train, X_test, = train_test_split(dataset,test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mX_train_scaled\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_scaled' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scale_datasets(X_train, X_test):\n",
    "  standard_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "  X_train_scaled = pd.DataFrame(\n",
    "      standard_scaler.fit_transform(X_train),\n",
    "      columns=X_train.columns\n",
    "  )\n",
    "  X_test_scaled = pd.DataFrame(\n",
    "      standard_scaler.transform(X_test),\n",
    "      columns = X_test.columns\n",
    "  )\n",
    "  return X_train_scaled, X_test_scaled\n",
    "X_train_scaled,X_test_scaled=scale_datasets(X_train,X_test) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        -156      -115      -140       149      -111      -334       -47  \\\n",
      "0   0.947079  0.973716  0.410453  0.234066  0.927332  0.912603  0.663357   \n",
      "1   0.926111  0.930108  0.171407  0.120879  0.906725  0.861902  0.535239   \n",
      "2   0.921118  0.930108  0.261337  0.258242  0.950108  0.879768  0.508113   \n",
      "3   0.930105  1.000000  0.431207  0.241758  0.723427  0.926123  0.604505   \n",
      "4   0.704943  0.813620  0.222905  0.164835  0.791757  0.837760  0.918624   \n",
      "5   0.767349  0.905018  0.286703  0.118681  0.744035  0.827137  0.814483   \n",
      "6   0.987019  0.764636  0.403536  0.264835  0.611714  0.565910  0.353354   \n",
      "7   0.981028  0.845878  0.913144  0.495604  0.345987  0.478996  0.824655   \n",
      "8   0.991013  0.970729  0.295926  0.148352  0.626898  0.723805  0.669654   \n",
      "9   0.942087  0.924134  0.301307  0.461538  0.670282  0.592467  0.300557   \n",
      "10  0.000000  0.000000  0.740968  0.736264  0.000000  0.000000  0.000000   \n",
      "11  0.588617  0.450418  0.218294  0.542857  0.514100  0.521004  0.863405   \n",
      "12  0.947079  0.994026  0.158340  0.181319  0.776573  0.864317  0.682974   \n",
      "13  0.877184  0.888889  0.149885  0.934066  0.067245  0.487687  0.809639   \n",
      "14  0.883175  0.798088  0.626441  0.451648  0.337310  0.379527  0.862194   \n",
      "15  0.942087  0.900239  0.325135  0.038462  0.793926  0.971511  0.628239   \n",
      "16  0.970544  0.925329  0.260569  0.038462  0.915401  1.000000  0.674497   \n",
      "17  0.884174  0.820789  0.283628  0.335165  0.300434  0.552873  0.273432   \n",
      "18  0.999501  0.942652  0.340507  0.060440  0.764642  0.750845  0.657302   \n",
      "19  0.799800  0.834528  0.300538  0.167033  0.292842  0.770642  0.375636   \n",
      "20  0.945082  0.976105  0.438893  0.040659  0.945770  0.809754  0.717607   \n",
      "21  1.000000  0.959976  0.489623  0.168132  0.804772  0.900531  0.637685   \n",
      "22  0.879181  0.939665  0.192929  0.136264  0.848156  0.787542  0.364495   \n",
      "23  0.930105  0.793907  0.375096  0.513187  0.459870  0.618542  0.373698   \n",
      "24  0.942087  0.927121  0.250576  0.180220  0.849241  0.913085  0.572051   \n",
      "25  0.271593  0.366786  0.538048  1.000000  0.592191  0.422501  0.742553   \n",
      "26  0.903645  0.919952  0.000000  0.701099  0.286334  0.711733  0.770889   \n",
      "27  0.916625  0.937276  0.239816  0.093407  0.739696  0.515693  0.889804   \n",
      "28  0.750374  0.744922  0.102998  0.283516  0.456616  0.845968  0.705013   \n",
      "29  0.924613  0.962963  0.267487  0.226374  0.786334  0.924674  0.540809   \n",
      "30  0.766850  0.864994  0.774789  0.310989  0.469631  0.583293  0.531848   \n",
      "31  0.707439  0.881720  0.217525  0.458242  1.000000  0.875905  0.502785   \n",
      "32  0.786321  0.646953  1.000000  0.564835  0.324295  0.648479  1.000000   \n",
      "33  0.945082  0.835723  0.502690  0.000000  0.260304  0.446161  0.885202   \n",
      "34  0.917124  0.882318  0.239047  0.204396  0.783080  0.870594  0.564543   \n",
      "\n",
      "        -204      -148       377  ...     172.9    1483.1     -10.9     182.9  \\\n",
      "0   0.986431  0.330028  0.511874  ...  0.323118  0.142815  0.815055  0.453602   \n",
      "1   0.914518  0.501416  0.495335  ...  0.270486  0.389088  0.818494  0.499904   \n",
      "2   0.811398  0.356941  0.470314  ...  0.417722  0.677996  0.805502  0.481076   \n",
      "3   0.938942  0.308782  0.469042  ...  0.275816  0.090787  0.816584  0.452450   \n",
      "4   0.898236  0.229462  0.632316  ...  0.514324  0.191711  0.754299  0.472238   \n",
      "5   0.896879  0.318697  0.470738  ...  0.349101  0.195587  0.715323  0.454179   \n",
      "6   0.158752  0.032578  0.503393  ...  0.730180  1.000000  0.792893  0.599616   \n",
      "7   0.534600  0.014164  0.446989  ...  0.296469  0.573196  0.847153  0.470893   \n",
      "8   0.903664  0.669972  0.548346  ...  0.312458  0.358676  0.798624  0.399616   \n",
      "9   0.001357  0.300283  0.410941  ...  0.565623  0.644007  0.772258  0.456868   \n",
      "10  0.069199  0.242210  0.246819  ...  0.681546  0.348092  0.501337  1.000000   \n",
      "11  0.172320  0.444759  0.574640  ...  0.439707  0.513864  0.790982  0.476657   \n",
      "12  0.724559  0.827195  0.542833  ...  0.249167  0.298151  0.833015  0.496830   \n",
      "13  0.853460  0.796034  0.539864  ...  0.089274  0.394753  1.000000  0.713160   \n",
      "14  0.332429  0.133144  0.863444  ...  0.357761  0.920841  0.722583  0.539097   \n",
      "15  0.782904  0.264873  0.516964  ...  0.392405  0.134317  0.646542  0.399232   \n",
      "16  0.940299  0.369688  0.480068  ...  0.367755  0.907424  0.825755  0.470125   \n",
      "17  0.483039  0.174221  0.610687  ...  0.550300  0.482558  0.852885  0.957541   \n",
      "18  0.759837  0.337110  0.571247  ...  0.409727  0.274001  0.917463  0.443228   \n",
      "19  0.930801  0.174221  0.523749  ...  0.566289  0.751789  0.253344  0.290106   \n",
      "20  0.951153  0.440510  0.488126  ...  0.408394  0.175909  0.817348  0.488569   \n",
      "21  0.928087  0.317280  0.516539  ...  0.408394  0.247168  0.882690  0.514121   \n",
      "22  0.818182  0.410765  1.000000  ...  0.465690  0.400268  0.823462  0.490874   \n",
      "23  0.629579  0.118980  0.377014  ...  0.352432  0.254472  0.857853  0.470509   \n",
      "24  1.000000  0.254958  0.556828  ...  0.534977  0.420394  0.803592  0.484534   \n",
      "25  0.936228  0.481586  0.539864  ...  0.447702  0.802624  0.406190  0.925841   \n",
      "26  0.670285  0.518414  0.649279  ...  1.000000  0.430233  0.904853  0.515658   \n",
      "27  0.857531  0.508499  0.470738  ...  0.348434  0.647734  0.716469  0.444573   \n",
      "28  0.592944  0.000000  0.575488  ...  0.455030  0.328265  0.000000  0.000000   \n",
      "29  0.867028  0.205382  0.468193  ...  0.375750  0.343023  0.832633  0.495677   \n",
      "30  0.613297  0.211048  0.037744  ...  0.464357  0.557841  0.577379  0.699520   \n",
      "31  1.000000  0.022663  0.486005  ...  0.315789  0.000000  0.456630  0.291835   \n",
      "32  0.000000  1.000000  0.097116  ...  0.000000  0.592427  0.904471  0.434006   \n",
      "33  0.200814  0.657224  0.000000  ...  0.769487  0.477639  0.627818  0.570221   \n",
      "34  0.810041  0.158640  0.505513  ...  0.425050  0.267591  0.739778  0.496830   \n",
      "\n",
      "       -16.3     472.1       1.7     27.16      51.6    -27.10  \n",
      "0   0.888105  0.179153  0.452910  0.882427  0.624196  0.960808  \n",
      "1   0.913306  0.093387  0.458156  0.847405  0.682878  0.979179  \n",
      "2   0.915827  0.156807  0.442418  0.865541  0.717846  0.966932  \n",
      "3   0.928427  0.118445  0.416687  0.861789  0.718248  0.977342  \n",
      "4   0.841734  0.204469  0.407694  0.774234  0.754823  0.944887  \n",
      "5   0.857359  0.229527  0.436173  0.779862  0.664791  0.838334  \n",
      "6   0.820060  0.377551  0.605296  0.949969  0.873392  0.955297  \n",
      "7   0.809476  0.814260  0.787659  0.818011  0.770900  0.958359  \n",
      "8   0.791331  0.292689  0.414439  0.899937  0.776527  0.972443  \n",
      "9   0.795867  0.528029  0.732950  0.839275  0.729904  0.966932  \n",
      "10  0.580141  0.525575  0.768673  0.432770  0.583601  0.734231  \n",
      "11  0.853831  0.642857  0.581064  0.787992  0.832797  0.879363  \n",
      "12  0.887097  0.168303  0.431426  0.845528  0.661174  0.976118  \n",
      "13  0.776714  0.334668  0.468649  0.781739  0.866559  0.984078  \n",
      "14  0.864415  0.946138  0.991257  0.881801  0.799437  1.000000  \n",
      "15  0.759577  0.115345  0.394704  0.640400  0.635048  0.803429  \n",
      "16  0.916331  0.137691  0.452161  0.868043  0.649920  0.987753  \n",
      "17  0.714718  0.576724  0.803897  0.759225  0.745177  0.958971  \n",
      "18  0.901714  0.272410  0.544841  0.853659  0.722267  0.976118  \n",
      "19  0.316028  0.280677  0.317012  0.220763  0.358119  0.187385  \n",
      "20  0.920363  0.193619  0.477642  0.831144  0.667605  0.968769  \n",
      "21  0.903730  0.202273  0.467899  0.828643  0.659164  0.962645  \n",
      "22  0.923387  0.151511  0.548339  0.894309  0.852090  0.991427  \n",
      "23  0.962198  0.239990  0.554834  0.888055  0.618971  0.946111  \n",
      "24  0.924395  0.161974  0.474144  0.899312  0.690113  0.973056  \n",
      "25  0.609879  0.911651  1.000000  0.505941  0.815916  0.599510  \n",
      "26  0.798891  0.317102  0.552835  1.000000  0.779743  0.990814  \n",
      "27  0.903226  0.517825  0.766425  0.934959  0.714228  0.987753  \n",
      "28  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "29  0.922883  0.162232  0.469148  0.871795  0.677653  0.993876  \n",
      "30  0.660786  1.000000  0.763927  0.705441  0.924839  0.780159  \n",
      "31  0.558972  0.209765  0.395953  0.472795  0.418408  0.549908  \n",
      "32  1.000000  0.625161  0.488384  0.876798  1.000000  0.780159  \n",
      "33  0.946069  0.946267  0.831626  0.786742  0.758842  0.792407  \n",
      "34  0.838710  0.147636  0.445666  0.724203  0.624598  0.893448  \n",
      "\n",
      "[35 rows x 7129 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "35/35 [==============================] - 44s 1s/step - loss: 0.0584 - mse: 0.0584 - val_loss: 0.0472 - val_mse: 0.0472\n",
      "Epoch 2/300\n",
      "35/35 [==============================] - 21s 591ms/step - loss: 0.0517 - mse: 0.0517 - val_loss: 0.0401 - val_mse: 0.0401\n",
      "Epoch 3/300\n",
      "35/35 [==============================] - 20s 580ms/step - loss: 0.0513 - mse: 0.0513 - val_loss: 0.0402 - val_mse: 0.0402\n",
      "Epoch 4/300\n",
      "35/35 [==============================] - 20s 562ms/step - loss: 0.0515 - mse: 0.0515 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 5/300\n",
      "35/35 [==============================] - 20s 571ms/step - loss: 0.0512 - mse: 0.0512 - val_loss: 0.0400 - val_mse: 0.0400\n",
      "Epoch 6/300\n",
      "35/35 [==============================] - 20s 579ms/step - loss: 0.0507 - mse: 0.0507 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 7/300\n",
      "35/35 [==============================] - 22s 644ms/step - loss: 0.0508 - mse: 0.0508 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 8/300\n",
      "35/35 [==============================] - 25s 707ms/step - loss: 0.0508 - mse: 0.0508 - val_loss: 0.0388 - val_mse: 0.0388\n",
      "Epoch 9/300\n",
      "35/35 [==============================] - 24s 691ms/step - loss: 0.0501 - mse: 0.0501 - val_loss: 0.0404 - val_mse: 0.0404\n",
      "Epoch 10/300\n",
      "35/35 [==============================] - 24s 693ms/step - loss: 0.0506 - mse: 0.0506 - val_loss: 0.0343 - val_mse: 0.0343\n",
      "Epoch 11/300\n",
      "35/35 [==============================] - 24s 689ms/step - loss: 0.0522 - mse: 0.0522 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 12/300\n",
      "35/35 [==============================] - 25s 705ms/step - loss: 0.0511 - mse: 0.0511 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 13/300\n",
      "35/35 [==============================] - 25s 702ms/step - loss: 0.0502 - mse: 0.0502 - val_loss: 0.0395 - val_mse: 0.0395\n",
      "Epoch 14/300\n",
      "35/35 [==============================] - 25s 705ms/step - loss: 0.0512 - mse: 0.0512 - val_loss: 0.0343 - val_mse: 0.0343\n",
      "Epoch 15/300\n",
      "35/35 [==============================] - 24s 680ms/step - loss: 0.0513 - mse: 0.0513 - val_loss: 0.0375 - val_mse: 0.0375\n",
      "Epoch 16/300\n",
      "35/35 [==============================] - 23s 667ms/step - loss: 0.0501 - mse: 0.0501 - val_loss: 0.0369 - val_mse: 0.0369\n",
      "Epoch 17/300\n",
      "35/35 [==============================] - 23s 663ms/step - loss: 0.0518 - mse: 0.0518 - val_loss: 0.0368 - val_mse: 0.0368\n",
      "Epoch 18/300\n",
      "35/35 [==============================] - 23s 666ms/step - loss: 0.0501 - mse: 0.0501 - val_loss: 0.0397 - val_mse: 0.0397\n",
      "Epoch 19/300\n",
      "35/35 [==============================] - 23s 658ms/step - loss: 0.0508 - mse: 0.0508 - val_loss: 0.0388 - val_mse: 0.0388\n",
      "Epoch 20/300\n",
      "35/35 [==============================] - 23s 664ms/step - loss: 0.0510 - mse: 0.0510 - val_loss: 0.0361 - val_mse: 0.0361\n",
      "Epoch 21/300\n",
      "35/35 [==============================] - 23s 654ms/step - loss: 0.0502 - mse: 0.0502 - val_loss: 0.0346 - val_mse: 0.0346\n",
      "Epoch 22/300\n",
      "35/35 [==============================] - 23s 665ms/step - loss: 0.0499 - mse: 0.0499 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 23/300\n",
      "35/35 [==============================] - 24s 673ms/step - loss: 0.0499 - mse: 0.0499 - val_loss: 0.0362 - val_mse: 0.0362\n",
      "Epoch 24/300\n",
      "35/35 [==============================] - 23s 657ms/step - loss: 0.0510 - mse: 0.0510 - val_loss: 0.0368 - val_mse: 0.0368\n",
      "Epoch 25/300\n",
      "35/35 [==============================] - 23s 661ms/step - loss: 0.0514 - mse: 0.0514 - val_loss: 0.0383 - val_mse: 0.0383\n",
      "Epoch 26/300\n",
      "35/35 [==============================] - 23s 668ms/step - loss: 0.0512 - mse: 0.0512 - val_loss: 0.0375 - val_mse: 0.0375\n",
      "Epoch 27/300\n",
      "35/35 [==============================] - 23s 661ms/step - loss: 0.0503 - mse: 0.0503 - val_loss: 0.0361 - val_mse: 0.0361\n",
      "Epoch 28/300\n",
      "35/35 [==============================] - 23s 644ms/step - loss: 0.0501 - mse: 0.0501 - val_loss: 0.0383 - val_mse: 0.0383\n",
      "Epoch 29/300\n",
      "35/35 [==============================] - 22s 635ms/step - loss: 0.0508 - mse: 0.0508 - val_loss: 0.0415 - val_mse: 0.0415\n",
      "Epoch 30/300\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.0501 - mse: 0.0501 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 31/300\n",
      "35/35 [==============================] - 22s 635ms/step - loss: 0.0500 - mse: 0.0500 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 32/300\n",
      "35/35 [==============================] - 23s 652ms/step - loss: 0.0504 - mse: 0.0504 - val_loss: 0.0368 - val_mse: 0.0368\n",
      "Epoch 33/300\n",
      "35/35 [==============================] - 22s 642ms/step - loss: 0.0505 - mse: 0.0505 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 34/300\n",
      "35/35 [==============================] - 22s 634ms/step - loss: 0.0501 - mse: 0.0501 - val_loss: 0.0382 - val_mse: 0.0382\n",
      "Epoch 35/300\n",
      "35/35 [==============================] - 23s 644ms/step - loss: 0.0505 - mse: 0.0505 - val_loss: 0.0365 - val_mse: 0.0365\n",
      "Epoch 36/300\n",
      "35/35 [==============================] - 23s 647ms/step - loss: 0.0508 - mse: 0.0508 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 37/300\n",
      "35/35 [==============================] - 23s 647ms/step - loss: 0.0504 - mse: 0.0504 - val_loss: 0.0377 - val_mse: 0.0377\n",
      "Epoch 38/300\n",
      "35/35 [==============================] - 23s 644ms/step - loss: 0.0496 - mse: 0.0496 - val_loss: 0.0362 - val_mse: 0.0362\n",
      "Epoch 39/300\n",
      "35/35 [==============================] - 23s 649ms/step - loss: 0.0497 - mse: 0.0497 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 40/300\n",
      "35/35 [==============================] - 22s 640ms/step - loss: 0.0507 - mse: 0.0507 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 41/300\n",
      "35/35 [==============================] - 22s 632ms/step - loss: 0.0507 - mse: 0.0507 - val_loss: 0.0361 - val_mse: 0.0361\n",
      "Epoch 42/300\n",
      "35/35 [==============================] - 22s 634ms/step - loss: 0.0513 - mse: 0.0513 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 43/300\n",
      "35/35 [==============================] - 22s 637ms/step - loss: 0.0506 - mse: 0.0506 - val_loss: 0.0377 - val_mse: 0.0377\n",
      "Epoch 44/300\n",
      "35/35 [==============================] - 22s 634ms/step - loss: 0.0494 - mse: 0.0494 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 45/300\n",
      "35/35 [==============================] - 22s 642ms/step - loss: 0.0505 - mse: 0.0505 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 46/300\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.0497 - mse: 0.0497 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 47/300\n",
      "35/35 [==============================] - 23s 646ms/step - loss: 0.0500 - mse: 0.0500 - val_loss: 0.0400 - val_mse: 0.0400\n",
      "Epoch 48/300\n",
      "35/35 [==============================] - 22s 633ms/step - loss: 0.0503 - mse: 0.0503 - val_loss: 0.0375 - val_mse: 0.0375\n",
      "Epoch 49/300\n",
      "35/35 [==============================] - 23s 643ms/step - loss: 0.0500 - mse: 0.0500 - val_loss: 0.0389 - val_mse: 0.0389\n",
      "Epoch 50/300\n",
      "35/35 [==============================] - 23s 644ms/step - loss: 0.0501 - mse: 0.0501 - val_loss: 0.0368 - val_mse: 0.0368\n",
      "Epoch 51/300\n",
      "35/35 [==============================] - 22s 634ms/step - loss: 0.0500 - mse: 0.0500 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 52/300\n",
      "35/35 [==============================] - 22s 639ms/step - loss: 0.0502 - mse: 0.0502 - val_loss: 0.0368 - val_mse: 0.0368\n",
      "Epoch 53/300\n",
      "35/35 [==============================] - 23s 659ms/step - loss: 0.0504 - mse: 0.0504 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 54/300\n",
      "35/35 [==============================] - 23s 650ms/step - loss: 0.0502 - mse: 0.0502 - val_loss: 0.0371 - val_mse: 0.0371\n",
      "Epoch 55/300\n",
      "35/35 [==============================] - 23s 652ms/step - loss: 0.0509 - mse: 0.0509 - val_loss: 0.0375 - val_mse: 0.0375\n",
      "Epoch 56/300\n",
      "35/35 [==============================] - 22s 636ms/step - loss: 0.0500 - mse: 0.0500 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 57/300\n",
      "35/35 [==============================] - 22s 641ms/step - loss: 0.0506 - mse: 0.0506 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 58/300\n",
      "35/35 [==============================] - 22s 638ms/step - loss: 0.0502 - mse: 0.0502 - val_loss: 0.0399 - val_mse: 0.0399\n",
      "Epoch 59/300\n",
      "35/35 [==============================] - 22s 637ms/step - loss: 0.0509 - mse: 0.0509 - val_loss: 0.0377 - val_mse: 0.0377\n",
      "Epoch 60/300\n",
      "35/35 [==============================] - 23s 646ms/step - loss: 0.0499 - mse: 0.0499 - val_loss: 0.0377 - val_mse: 0.0377\n",
      "Epoch 61/300\n",
      "35/35 [==============================] - 22s 643ms/step - loss: 0.0498 - mse: 0.0498 - val_loss: 0.0359 - val_mse: 0.0359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/300\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.0508 - mse: 0.0508 - val_loss: 0.0398 - val_mse: 0.0398\n",
      "Epoch 63/300\n",
      "35/35 [==============================] - 22s 643ms/step - loss: 0.0505 - mse: 0.0505 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 64/300\n",
      "35/35 [==============================] - 24s 675ms/step - loss: 0.0509 - mse: 0.0509 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 65/300\n",
      "35/35 [==============================] - 24s 683ms/step - loss: 0.0500 - mse: 0.0500 - val_loss: 0.0396 - val_mse: 0.0396\n",
      "Epoch 66/300\n",
      "35/35 [==============================] - 23s 663ms/step - loss: 0.0498 - mse: 0.0498 - val_loss: 0.0372 - val_mse: 0.0372\n",
      "Epoch 67/300\n",
      "35/35 [==============================] - 23s 659ms/step - loss: 0.0497 - mse: 0.0497 - val_loss: 0.0379 - val_mse: 0.0379\n",
      "Epoch 68/300\n",
      "35/35 [==============================] - 23s 667ms/step - loss: 0.0504 - mse: 0.0504 - val_loss: 0.0346 - val_mse: 0.0346\n",
      "Epoch 69/300\n",
      "35/35 [==============================] - 23s 659ms/step - loss: 0.0499 - mse: 0.0499 - val_loss: 0.0371 - val_mse: 0.0371\n",
      "Epoch 70/300\n",
      "35/35 [==============================] - 23s 667ms/step - loss: 0.0499 - mse: 0.0499 - val_loss: 0.0366 - val_mse: 0.0366\n",
      "Epoch 71/300\n",
      "35/35 [==============================] - 23s 660ms/step - loss: 0.0508 - mse: 0.0508 - val_loss: 0.0378 - val_mse: 0.0378\n",
      "Epoch 72/300\n",
      "35/35 [==============================] - 23s 670ms/step - loss: 0.0500 - mse: 0.0500 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 73/300\n",
      "35/35 [==============================] - 23s 670ms/step - loss: 0.0503 - mse: 0.0503 - val_loss: 0.0373 - val_mse: 0.0373\n",
      "Epoch 74/300\n",
      "35/35 [==============================] - 23s 657ms/step - loss: 0.0503 - mse: 0.0503 - val_loss: 0.0372 - val_mse: 0.0372\n",
      "Epoch 75/300\n",
      "35/35 [==============================] - 23s 663ms/step - loss: 0.0500 - mse: 0.0500 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 76/300\n",
      "35/35 [==============================] - 23s 656ms/step - loss: 0.0511 - mse: 0.0511 - val_loss: 0.0366 - val_mse: 0.0366\n",
      "Epoch 77/300\n",
      "35/35 [==============================] - 24s 676ms/step - loss: 0.0499 - mse: 0.0499 - val_loss: 0.0361 - val_mse: 0.0361\n",
      "Epoch 78/300\n",
      "35/35 [==============================] - 23s 664ms/step - loss: 0.0496 - mse: 0.0496 - val_loss: 0.0374 - val_mse: 0.0374\n",
      "Epoch 79/300\n",
      "35/35 [==============================] - 23s 661ms/step - loss: 0.0500 - mse: 0.0500 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 80/300\n",
      "35/35 [==============================] - 23s 657ms/step - loss: 0.0493 - mse: 0.0493 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 81/300\n",
      "35/35 [==============================] - 23s 670ms/step - loss: 0.0498 - mse: 0.0498 - val_loss: 0.0384 - val_mse: 0.0384\n",
      "Epoch 82/300\n",
      "35/35 [==============================] - 23s 661ms/step - loss: 0.0497 - mse: 0.0497 - val_loss: 0.0377 - val_mse: 0.0377\n",
      "Epoch 83/300\n",
      "35/35 [==============================] - 23s 662ms/step - loss: 0.0502 - mse: 0.0502 - val_loss: 0.0404 - val_mse: 0.0404\n",
      "Epoch 84/300\n",
      "35/35 [==============================] - 23s 667ms/step - loss: 0.0500 - mse: 0.0500 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 85/300\n",
      "35/35 [==============================] - 23s 671ms/step - loss: 0.0496 - mse: 0.0496 - val_loss: 0.0366 - val_mse: 0.0366\n",
      "Epoch 86/300\n",
      "35/35 [==============================] - 23s 665ms/step - loss: 0.0499 - mse: 0.0499 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 87/300\n",
      "35/35 [==============================] - 23s 665ms/step - loss: 0.0503 - mse: 0.0503 - val_loss: 0.0379 - val_mse: 0.0379\n",
      "Epoch 88/300\n",
      "35/35 [==============================] - 23s 664ms/step - loss: 0.0520 - mse: 0.0520 - val_loss: 0.0362 - val_mse: 0.0362\n",
      "Epoch 89/300\n",
      "35/35 [==============================] - 23s 669ms/step - loss: 0.0500 - mse: 0.0500 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 90/300\n",
      "35/35 [==============================] - 23s 658ms/step - loss: 0.0497 - mse: 0.0497 - val_loss: 0.0361 - val_mse: 0.0361\n",
      "Epoch 91/300\n",
      "35/35 [==============================] - 23s 645ms/step - loss: 0.0499 - mse: 0.0499 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 92/300\n",
      "35/35 [==============================] - 23s 646ms/step - loss: 0.0499 - mse: 0.0499 - val_loss: 0.0366 - val_mse: 0.0366\n",
      "Epoch 93/300\n",
      "35/35 [==============================] - 23s 652ms/step - loss: 0.0503 - mse: 0.0503 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 94/300\n",
      "35/35 [==============================] - 22s 638ms/step - loss: 0.0500 - mse: 0.0500 - val_loss: 0.0375 - val_mse: 0.0375\n",
      "Epoch 95/300\n",
      "35/35 [==============================] - 22s 643ms/step - loss: 0.0497 - mse: 0.0497 - val_loss: 0.0372 - val_mse: 0.0372\n",
      "Epoch 96/300\n",
      "35/35 [==============================] - 22s 641ms/step - loss: 0.0494 - mse: 0.0494 - val_loss: 0.0365 - val_mse: 0.0365\n",
      "Epoch 97/300\n",
      "35/35 [==============================] - 23s 657ms/step - loss: 0.0502 - mse: 0.0502 - val_loss: 0.0365 - val_mse: 0.0365\n",
      "Epoch 98/300\n",
      "35/35 [==============================] - 22s 639ms/step - loss: 0.0503 - mse: 0.0503 - val_loss: 0.0388 - val_mse: 0.0388\n",
      "Epoch 99/300\n",
      "35/35 [==============================] - 22s 638ms/step - loss: 0.0496 - mse: 0.0496 - val_loss: 0.0379 - val_mse: 0.0379\n",
      "Epoch 100/300\n",
      "35/35 [==============================] - 22s 641ms/step - loss: 0.0496 - mse: 0.0496 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 101/300\n",
      "35/35 [==============================] - 22s 641ms/step - loss: 0.0502 - mse: 0.0502 - val_loss: 0.0373 - val_mse: 0.0373\n",
      "Epoch 102/300\n",
      "35/35 [==============================] - 22s 641ms/step - loss: 0.0496 - mse: 0.0496 - val_loss: 0.0365 - val_mse: 0.0365\n",
      "Epoch 103/300\n",
      "35/35 [==============================] - 22s 628ms/step - loss: 0.0498 - mse: 0.0498 - val_loss: 0.0402 - val_mse: 0.0402\n",
      "Epoch 104/300\n",
      "35/35 [==============================] - 23s 647ms/step - loss: 0.0497 - mse: 0.0497 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 105/300\n",
      "35/35 [==============================] - 23s 655ms/step - loss: 0.0499 - mse: 0.0499 - val_loss: 0.0373 - val_mse: 0.0373\n",
      "Epoch 106/300\n",
      "35/35 [==============================] - 23s 646ms/step - loss: 0.0501 - mse: 0.0501 - val_loss: 0.0368 - val_mse: 0.0368\n",
      "Epoch 107/300\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.0504 - mse: 0.0504 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 108/300\n",
      "35/35 [==============================] - 23s 643ms/step - loss: 0.0500 - mse: 0.0500 - val_loss: 0.0384 - val_mse: 0.0384\n",
      "Epoch 109/300\n",
      "35/35 [==============================] - 23s 657ms/step - loss: 0.0499 - mse: 0.0499 - val_loss: 0.0373 - val_mse: 0.0373\n",
      "Epoch 110/300\n",
      "35/35 [==============================] - 23s 666ms/step - loss: 0.0495 - mse: 0.0495 - val_loss: 0.0371 - val_mse: 0.0371\n",
      "Epoch 111/300\n",
      "35/35 [==============================] - 23s 671ms/step - loss: 0.0495 - mse: 0.0495 - val_loss: 0.0373 - val_mse: 0.0373\n",
      "Epoch 112/300\n",
      "35/35 [==============================] - 23s 668ms/step - loss: 0.0507 - mse: 0.0507 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 113/300\n",
      "32/35 [==========================>...] - ETA: 1s - loss: 0.0515 - mse: 0.0515"
     ]
    }
   ],
   "source": [
    "class AutoEncoders(Model):\n",
    "\n",
    "  def __init__(self, output_units):\n",
    "\n",
    "    super().__init__()\n",
    "    \n",
    "    self.encoder = Sequential(\n",
    "        [\n",
    "          Dense(5000, activation=\"tanh\"),\n",
    "          Dense(3000, activation=\"tanh\"),\n",
    "          Dense(1000, activation=\"tanh\"),\n",
    "          Dense(500, activation=\"tanh\"),\n",
    "          Dense(100, activation=\"tanh\"),\n",
    "          Dense(50, activation=\"tanh\"),\n",
    "          Dense(15, activation=\"tanh\")\n",
    "        ]\n",
    "    )\n",
    "    self.decoder = Sequential(\n",
    "        [\n",
    "            Dense(50, activation=\"sigmoid\"),\n",
    "          Dense(100, activation=\"sigmoid\"),\n",
    "          Dense(500, activation=\"sigmoid\"),\n",
    "          Dense(1000, activation=\"sigmoid\"),\n",
    "          Dense(3000, activation=\"sigmoid\"),\n",
    "          Dense(5000, activation=\"sigmoid\"),\n",
    "          Dense(output_units, activation=\"sigmoid\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "  def call(self, inputs):\n",
    "\n",
    "    encoded = self.encoder(inputs)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "auto_encoder = AutoEncoders(len(X_train_scaled.columns))\n",
    "\n",
    "auto_encoder.compile(\n",
    "    loss='mse',\n",
    "    metrics=['mse'],\n",
    "    optimizer='adam'\n",
    "    \n",
    "  )\n",
    "history = auto_encoder.fit(\n",
    "      X_train_scaled, \n",
    "      X_train_scaled,\n",
    "      epochs=300, \n",
    "      batch_size=1, \n",
    "      validation_data=(X_test_scaled, X_test_scaled)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No such layer: sequential_2. Existing layers are: ['sequential_8', 'sequential_9'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m encoder_layer \u001b[38;5;241m=\u001b[39m \u001b[43mauto_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msequential_2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m reduced_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(encoder_layer\u001b[38;5;241m.\u001b[39mpredict(X_train_scaled))\n\u001b[0;32m      3\u001b[0m reduced_df \u001b[38;5;241m=\u001b[39m reduced_df\u001b[38;5;241m.\u001b[39madd_prefix(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:3275\u001b[0m, in \u001b[0;36mModel.get_layer\u001b[1;34m(self, name, index)\u001b[0m\n\u001b[0;32m   3273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m name:\n\u001b[0;32m   3274\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m layer\n\u001b[1;32m-> 3275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3276\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such layer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Existing layers are: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(layer\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3278\u001b[0m     )\n\u001b[0;32m   3279\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3280\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvide either a layer name or layer index at \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`get_layer`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3281\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: No such layer: sequential_2. Existing layers are: ['sequential_8', 'sequential_9']."
     ]
    }
   ],
   "source": [
    "encoder_layer = auto_encoder.get_layer('sequential_2')\n",
    "reduced_df = pd.DataFrame(encoder_layer.predict(X_train_scaled))\n",
    "reduced_df = reduced_df.add_prefix('feature_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0    0.987121   0.973766   0.985543   0.248708   0.981070   0.998433   \n",
      "1    0.986936   0.973373   0.986025   0.268485   0.981555   0.998396   \n",
      "2    0.987456   0.974787   0.983966   0.180210   0.979671   0.998525   \n",
      "3    0.987444   0.974722   0.984019   0.182173   0.979788   0.998520   \n",
      "4    0.986650   0.972503   0.986981   0.316015   0.982650   0.998310   \n",
      "..        ...        ...        ...        ...        ...        ...   \n",
      "65   0.987452   0.974768   0.983860   0.175700   0.979733   0.998524   \n",
      "66   0.987550   0.975067   0.983362   0.155535   0.979256   0.998549   \n",
      "67   0.987544   0.975040   0.983338   0.155442   0.979317   0.998547   \n",
      "68   0.987111   0.973772   0.985557   0.246074   0.980986   0.998436   \n",
      "69   0.987516   0.975014   0.983483   0.160736   0.979373   0.998542   \n",
      "\n",
      "    feature_6  feature_7  feature_8  feature_9  feature_10  feature_11  \\\n",
      "0   -0.968814   0.996773  -0.968948   0.991416   -0.996788    0.971795   \n",
      "1   -0.968808   0.996691  -0.969205   0.991229   -0.996805    0.971727   \n",
      "2   -0.968536   0.997004  -0.967821   0.991897   -0.996807    0.972096   \n",
      "3   -0.968527   0.996985  -0.967796   0.991877   -0.996801    0.971993   \n",
      "4   -0.968701   0.996506  -0.969861   0.990889   -0.996809    0.971427   \n",
      "..        ...        ...        ...        ...         ...         ...   \n",
      "65  -0.968377   0.997008  -0.967595   0.991929   -0.996818    0.972066   \n",
      "66  -0.968324   0.997073  -0.967329   0.992053   -0.996818    0.972146   \n",
      "67  -0.968267   0.997067  -0.967250   0.992064   -0.996820    0.972107   \n",
      "68  -0.968842   0.996778  -0.968941   0.991396   -0.996793    0.971839   \n",
      "69  -0.968358   0.997059  -0.967433   0.992021   -0.996820    0.972134   \n",
      "\n",
      "    feature_12  feature_13  feature_14  \n",
      "0    -0.964871    0.995032    0.994657  \n",
      "1    -0.964500    0.995088    0.994815  \n",
      "2    -0.965990    0.994893    0.994145  \n",
      "3    -0.965969    0.994882    0.994157  \n",
      "4    -0.963683    0.995187    0.995146  \n",
      "..         ...         ...         ...  \n",
      "65   -0.966084    0.994885    0.994120  \n",
      "66   -0.966418    0.994834    0.993961  \n",
      "67   -0.966423    0.994827    0.993955  \n",
      "68   -0.964858    0.995044    0.994649  \n",
      "69   -0.966339    0.994845    0.994001  \n",
      "\n",
      "[70 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "print(reduced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5addf786bcd861d1ce5006f23111f8cbb206731e5b61b0a5632ba9e0252558a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
